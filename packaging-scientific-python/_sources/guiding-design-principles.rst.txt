=========================
Guiding Design Principles
=========================

In this section we summarize some guiding principles for designing and
organizing scientific Python code.

Collaborate
-----------

Software developed by several people is preferable to software developed by
one.

Talking through a design and the assumptions in it helps to clarify your
thinking. To quote the
`Zen of Python <https://www.python.org/dev/peps/pep-0020/#id3>`_,
"If the implementation is hard to explain, it's a bad idea. If the
implementation is easy to explain, it may be a good idea."

Collaboration takes trust. It is OK to be "wrong"; it is part of the process
of making things better.

Having more than one person understanding every part of the code prevents
systematic risks for the project and keeps you from being tied to that code.

If you can bring together contributors with diverse scientific backgrounds, it
becomes easier to identify functionality that should be generalized for reuse
by different fields.

Don't Be Afraid to Refactor
---------------------------

No code is ever right the first (or second) time.

Refactoring the code once you understand the problem and the design trade-offs
more fully helps keep the code maintainable.

Prefer "Wide" over "Deep"
-------------------------

It should be possible to reuse pieces of software in a way not anticipated by
the original author. That is, branching out from the initial use case should
enable unplanned functionality without a massive increase in complexity.

When building new things, work your way down to the lowest level, understand
that level, and then build back up. Try to imagine what else you would want to
do with the capability you are implementing for other research groups, for
related scientific applications, and next year.

Take the time to understand how things need to work at the bottom. It is better
to slowly deploy a robust extensible solution than to quickly deploy a brittle
narrow solution.

Keep I/O Separate
-----------------

One of the biggest impediments to reuse of scientific code is when I/O
code---assuming certain file locations, names, formats, or layouts---is
interspersed with scientific logic.

I/O-related functions should *only* perform I/O. For example, they should take
in a filepath and return a numpy array, or a dictionary of arrays and metadata.
The valuable scientific logic should be encoded in functions that take in
standard data types and return standard data types. This makes them easier to
test, maintain when data formats change, or reuse for unforeseen applications.

Duck Typing is a Good Idea
--------------------------

`Duck typing <https://en.wikipedia.org/wiki/Duck_typing>`_ treats objects based
on what they can *do*, not based on what type they *are*.

Python in general and scientific Python in particular leverage *interfaces* to
support reuse. For example, it is possible to pass a pandas DataFrame to the
:func:`numpy.sum` function even though pandas was created long after
:func:`numpy.sum`. This is because :func:`numpy.sum` avoids assuming it will
be passed specific data types; it accepts any object that provides the right
methods (interfaces). Where possible, avoid ``isinstance`` checks in your
code, and try to make your functions work on the broadest possible range of
input types.

"Stop Writing Classes"
----------------------

It is often tempting to invent special objects for a use case or workflow ---
an ``Image`` object or a ``DiffractionAnalysis`` object. This approach has
proven again and again to be difficult to extend and maintain. It is better to
prefer standard, simple data structures like Python dictionaries and numpy
arrays and use simple functions to operate on them.

A popular talk, "Stop Writing Classes," which you can
`watch on YouTube <https://www.youtube.com/watch?v=o9pEzgHorH0&t=193s>`_
illustrates how some situations that *seem* to lend themselves to
object-oriented programming are much more simply handled using plain, built-in
data structures and functions.

As another example, the widely-used scikit-image library initially experimented
with using an ``Image`` class, but ultimately decided that it was better to use
plain old numpy arrays. All scientific Python libraries understand numpy
arrays, but they don't understand custom classes, so it is better to pass
application-specific metadata *alongside* a standard array than to try to
encapsulate all of that information in a new, bespoke object.

Complexity is Always Conserved
------------------------------

Complexity is always conserved and is strictly greater than the system the code
is modeling. Attempts to hide complexity from the user frequently backfire.

For example, it is often tempting to hide certain reused keywords in a
function. It might be tempting to shorten this:

.. code-block:: python

    get_image(filename, normalize=True, beginning=0, end=None):
        ...

into this:

.. code-block:: python

    def get_image(filename, options={}):
        ...

Although the complexity appears to have been reduced through hidden keyword
arguments, it has been slightly complicated through the need to dig through
more documentation to better understand how to use them.

Because new science occurs when old ideas are reapplied or extended in
unforeseen ways, scientific code should not bury its complexity or overly
optimize for a specific use case. It should expose what complexity there is
straightforwardly.
